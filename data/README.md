

# Data

## Original Data

We directly upload the original data to corresponding folders.

Something you may note is:

1. For CoNaLa, we use the built-in function `load_dataset` in transformers library
to automatically download the dataset.
2. For MBJP, it is originally used for the evaluation of code generation.
In its original dataset, the function head is provided as an input. 
We use `process.py` to combine them together. `mbjp.json` is an already
processed version.
3. For APPS, it is also a dataset that is originally used for code generation. 
And for each description there are several correct answers. 
We randomly sample one answer for each description. And we only save those 
code snippets that are not longer than 512. This process is implemented in `random_sample.py`.
As mentioned in our paper, we then randomly select 
a portion of pairs from the training and test sets. `ori-query.json` and 
`ori-code.json` are the files we use in our experiment.

## Exemplar Code Generation & Code Rewritting

We provide some examples generated by Code Llama-7b in the folder of each dataset.

### GPT 3.5
The generation and rewritting process is in `gpt35.py`.

To generate exempalr code, run:
```angular2html
python gpt35.py --apikey YOUR_KEY \
--save_interval 5 --file_name FILE_NAME --dataset YOUR_DATASET \
--n_gen 4
```
Here `--apikey` is your OpenAI's API Key, `--dataset` is chosen from
`[conala, mbpp, apps, mbjp]`, and `--n_gen` refers to the number of exemplar codes
you want to generate for the same query.

To rewrite the code, as described in the paper, we first summarize the code into a description
and then ask LLMs to generate code based on that. Simply add `--sum_first` to the argument
and the program will do the both.

For the detail of creating OpenAI's API and pricing information,
please refer to [OpenAI's website](https://openai.com/product).

### Code Llama
The generation and rewritting process is in `codellama.py`.
The whole working pipeline is the same as GPT 3.5.

To generate using a Code Llama-7b model, run:
```angular2html
srun torchrun --nproc_per_node 1 --master_port 29501 \
example_instructions.py --ckpt_dir CodeLlama-7b-Instruct/ \
--tokenizer_path CodeLlama-7b-Instruct/tokenizer.model \
--max_seq_len 4096 --max_batch_size 4 --gen_time 4 \
--file_name FILE_NAME --dataset YOUR_DATASET
```

Using 13b or 34b model is similar to 7b, just replacing the `--ckpt_dir` and 
`--tokenizer_path` accordingly. And note that you have to change `--nproc_per_node ` accordingly. `--dataset` is chosen from
`[conala, mbpp, apps, mbjp]`, and `--gen_time` refers to the number of exemplar codes
you want to generate for the same query. You can also use `--sum_first` to rewrite the code.

For the downloading of Code Llama's parameters and other deployment details.
Please refer to Code Llama's [project page](https://github.com/facebookresearch/codellama).


